{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081e09c0-52a8-459c-8a02-07ac8050df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import net\n",
    "from function import adaptive_instance_normalization, coral\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eff2661-8f08-457f-9352-e5700ef8b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_image(content,style):\n",
    "# No interpolation in this\n",
    "\n",
    "    def test_transform(size, crop):\n",
    "        transform_list = []\n",
    "        if size != 0:\n",
    "            transform_list.append(transforms.Resize(size))\n",
    "        if crop:\n",
    "            transform_list.append(transforms.CenterCrop(size))\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "        transform = transforms.Compose(transform_list)\n",
    "        return transform\n",
    "\n",
    "\n",
    "    def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
    "                       interpolation_weights=None):\n",
    "        assert (0.0 <= alpha <= 1.0)\n",
    "        content_f = vgg(content)\n",
    "        style_f = vgg(style)\n",
    "        if interpolation_weights:\n",
    "            _, C, H, W = content_f.size()\n",
    "            feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n",
    "            base_feat = adaptive_instance_normalization(content_f, style_f)\n",
    "            for i, w in enumerate(interpolation_weights):\n",
    "                feat = feat + w * base_feat[i:i + 1]\n",
    "            content_f = content_f[0:1]\n",
    "        else:\n",
    "            feat = adaptive_instance_normalization(content_f, style_f)\n",
    "        feat = feat * alpha + content_f * (1 - alpha)\n",
    "        return decoder(feat)\n",
    "\n",
    "    # #basic Informations\n",
    "    # parser.add_argument('--content', type=str,\n",
    "    #                     help='File path to the content image')\n",
    "    # parser.add_argument('--style', type=str,\n",
    "    #                     help='File path to the style image, or multiple style \\\n",
    "    #                     images separated by commas if you want to do style \\\n",
    "    #                     interpolation or spatial control')\n",
    "    \n",
    "    \n",
    "    vgg_path='models/vgg_normalised.pth'\n",
    "    decoder_path='models/decoder.pth'\n",
    "    \n",
    "    # Additional options\n",
    "    content_size=512\n",
    "    style_size=512\n",
    "    crop=False\n",
    "    save_ext='.jpg'\n",
    "    output='output'\n",
    "    \n",
    "    # Advanced options\n",
    "    preserve_color=False # To preserve colour of content image\n",
    "    alpha=1.0 # The weight that controls the degree of \\stylization. Should be between 0 and 1\n",
    "    \n",
    "    \n",
    "    # parser.add_argument(\n",
    "    #     '--style_interpolation_weights', type=str, default='',\n",
    "    #     help='The weight for blending the style of multiple style images')\n",
    "    \n",
    "    do_interpolation = False\n",
    "    \n",
    "    output_dir = Path(output)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    assert (content)\n",
    "    if content:\n",
    "        content_paths = [Path(content)]\n",
    "    \n",
    "    assert (style)\n",
    "    if style:\n",
    "        style_paths = style.split(',')\n",
    "        if len(style_paths) == 1:\n",
    "            style_paths = [Path(style)]\n",
    "    \n",
    "    decoder = net.decoder\n",
    "    vgg = net.vgg\n",
    "    \n",
    "    decoder.eval()\n",
    "    vgg.eval()\n",
    "    \n",
    "    decoder.load_state_dict(torch.load(decoder_path))\n",
    "    vgg.load_state_dict(torch.load(vgg_path))\n",
    "    vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "    \n",
    "    vgg.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    content_tf = test_transform(content_size, crop)\n",
    "    style_tf = test_transform(style_size, crop)\n",
    "    \n",
    "    for content_path in content_paths:\n",
    "        if do_interpolation:  # one content image, N style image, not used, let it be for now\n",
    "            style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "            content = content_tf(Image.open(str(content_path))).unsqueeze(0).expand_as(style)\n",
    "            style = style.to(device)\n",
    "            content = content.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(vgg, decoder, content, style,alpha, interpolation_weights)\n",
    "            output = output.cpu()\n",
    "            output_name = output_dir / '{:s}_interpolation{:s}'.format(\n",
    "                content_path.stem, save_ext)\n",
    "\n",
    "            transform = transforms.ToPILImage()\n",
    "            output=transform(output[0])\n",
    "    \n",
    "        else:  # process one content and one style\n",
    "            for style_path in style_paths:\n",
    "                content = content_tf(Image.open(str(content_path)))\n",
    "                style = style_tf(Image.open(str(style_path)))\n",
    "                if preserve_color:\n",
    "                    style = coral(style, content)\n",
    "                style = style.to(device).unsqueeze(0)\n",
    "                content = content.to(device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    output = style_transfer(vgg, decoder, content, style,alpha)\n",
    "                output = output.cpu()\n",
    "    \n",
    "                output_name = output_dir / '{:s}_stylized_{:s}{:s}'.format(\n",
    "                    content_path.stem, style_path.stem, save_ext)\n",
    "                transform = transforms.ToPILImage()\n",
    "                output=transform(output[0])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "app = gr.Interface(\n",
    "    fn=show_image,\n",
    "    inputs=[gr.Image(label=\"Content Image\", type=\"filepath\"),gr.Image(label=\"Style Image\", type='filepath')],\n",
    "    outputs=gr.Image(label=\"Output Image\", type=\"filepath\"),\n",
    ")\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f7681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
